<!-- ---
title: "Selected Projects"
permalink: /projects/
author_profile: true
---

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>b7b86163aa384d4490f7a738b83da382</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="train-a-cellular-attention-network-can"
class="cell markdown">
<h1>Train a Cellular Attention Network (CAN)</h1>
<p>Cellular complexes are graphs that consisted of nodes,edges, and
faces. Intuitively, Cellular Attention Networks work by learning edge
features via a combination of incident edges that share a node, and
incident edges that share a face.This implementation was contributed to
the topological deep learning library <a
href="https://github.com/pyt-team/TopoModelX/">TopoModelX</a>.</p>
<p>We create and train a neural network for cellular complexes with
layers using a message passing scheme provided by the down and up
Laplacians as proposed in <a
href="https://arxiv.org/pdf/2110.05614.pdf">Rodenberry et. al: Signal
processing on cell complexes (2022)</a>. We also train layers utilizing
the cell attention mechanism originally proposed in <a
href="https://arxiv.org/abs/2209.08179">Giusti et. al: Cell Attention
Networks (2022)</a>. Intuitively updates are made</p>
<!-- We create and train a simplified version of the CCXN originally proposed in [Hajij et. al : Cell Complex Neural Networks (2020)](https://arxiv.org/pdf/2010.00743.pdf). -->
<h3 id="the-neural-network">The Neural Network:</h3>
<p>The equations of one layer of this neural network without the
attention mechanism are given by:</p>
<ul>
<li>A convolution from edges to edges using the down and up laplacian to
pass messages:</li>
</ul>
<p>🟥 <span
class="math inline">  <em>m</em><sub><em>y</em> → {<em>z</em>} → <em>x</em></sub><sup>(1→0→1)</sup> = <em>L</em><sub>↓,1</sub> ⋅ <em>h</em><sub><em>y</em></sub><sup><em>t</em>, (1)</sup> ⋅ <em>Θ</em><sup><em>t</em>, (1→0→1)</sup></span></p>
<p>🟥 <span
class="math inline">  <em>m</em><sub><em>y</em> → {<em>z</em>} → <em>x</em></sub><sup>(1→2→1)</sup> = <em>L</em><sub>↑,1</sub> ⋅ <em>h</em><sub><em>y</em></sub><sup><em>t</em>, (1)</sup> ⋅ <em>Θ</em><sup><em>t</em>, (1→2→1)</sup></span></p>
<p>🟥 <span
class="math inline">  <em>m</em><sub><em>x</em> → <em>x</em></sub><sup>(1→1)</sup> = <em>h</em><sub><em>x</em></sub><sup><em>t</em>, (1)</sup> ⋅ <em>Θ</em><sup><em>t</em>, (1→1)</sup></span></p>
<p>🟧 <span
class="math inline">  <em>m</em><sub><em>x</em></sub><sup>(1→0→1)</sup> = ∑<sub><em>y</em> ∈ ℬ(<em>x</em>)</sub><em>m</em><sub><em>y</em> → <em>x</em></sub><sup>(1→0→1)</sup></span></p>
<p>🟧 <span
class="math inline">  <em>m</em><sub><em>x</em></sub><sup>(1→2→1)</sup> = ∑<sub><em>y</em> ∈ 𝒞(<em>x</em>)</sub><em>m</em><sub><em>y</em> → <em>x</em></sub><sup>(1→2→1)</sup></span></p>
<p>🟩: <span
class="math inline">  <em>m</em><sub><em>x</em></sub><sup>(1)</sup> = <em>m</em><sub><em>x</em></sub><sup>(1→0→1)</sup> + <em>m</em><sub><em>x</em> → <em>x</em></sub><sup>(1→1)</sup> + <em>m</em><sub><em>x</em></sub><sup>(1→2→1)</sup></span></p>
<p>🟦 <span
class="math inline">  <em>h</em><sub><em>x</em></sub><sup><em>t</em> + 1, (1)</sup> = <em>σ</em>(<em>m</em><sub><em>x</em></sub><sup>(1)</sup>)</span></p>
<!-- The equations of one layer of this neural network with the attention mechanism are given by: masked by the up and down Laplacian are given by: -->
<!-- - A convolution from edges to edges using an attention mechanism masked by the down and up Laplacians:

🟥 $\quad m_{y \rightarrow \{z\} \rightarrow x}^{(1 \rightarrow 2 \rightarrow 1)} = (L_{\uparrow,1} \odot att(h_{y \in \mathcal{L}\uparrow(x)}^{t,(1)}, h_x^{t,(1)}))_{xy} \cdot h_y^{t,(1)} \cdot \Theta^{t,(1 \rightarrow 2 \rightarrow 1)} $ 

🟥 $\quad m_{y \rightarrow \{z\} \rightarrow x}^{(1 \rightarrow 0 \rightarrow 1)} = (L_{\downarrow,1} \odot att(h_{y \in \mathcal{L}\downarrow(x)}^{t,(1)}, h_x^{t,(1)}))_{xy} \cdot h_y^{t,(1)} \cdot \Theta^{t,(1 \rightarrow 0 \rightarrow 1)}$ 

🟥 $\quad m^{(1 \rightarrow 1)}_{x \rightarrow x} = (1+\epsilon)\cdot h_x^{t, (1)} \cdot \Theta^{t,(1 \rightarrow 1)} $ 

🟧 $\quad m_{x}^{(1 \rightarrow 2 \rightarrow 1)} = \sum_{y \in \mathcal{L}_\uparrow(x)}m_{y \rightarrow \{z\} \rightarrow x}^{(1 \rightarrow 2 \rightarrow 1)}$ 

🟧 $\quad m_{x}^{(1 \rightarrow 0 \rightarrow 1)} = \sum_{y \in \mathcal{L}_\downarrow(x)}m_{y \rightarrow \{z\} \rightarrow x}^{(1 \rightarrow 0 \rightarrow 1)}$ 

🟧 $\quad m^{(1 \rightarrow 1)}_{x} = m^{(1 \rightarrow 1)}_{x \rightarrow x}$ 

🟩 $\quad m_x^{(1)} = m_x^{(1 \rightarrow 1)} + m_{x}^{(1 \rightarrow 2 \rightarrow 1)} + m_{x}^{(1 \rightarrow 0 \rightarrow 1)}$ 

🟦 $\quad h_x^{t+1, (1)} = \sigma(\theta_{att} \cdot m_x^{(1)})\cdot \sigma(m_x^{(1)})$ -->
<p>Where the notations are defined in <a
href="https://arxiv.org/abs/2304.10031">Papillon et al : Architectures
of Topological Deep Learning: A Survey of Topological Neural Networks
(2023)</a>.</p>
<h3 id="the-task">The Task:</h3>
<p>We train this model to perform entire complex classification on a
small version of <a
href="http://shapenet.cs.stanford.edu/shrec16/">shrec16</a>.</p>
</section>
<section id="set-up" class="cell markdown">
<h1>Set-up</h1>
</section>
<div class="cell code" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> toponetx.datasets <span class="im">as</span> datasets</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> topomodelx.nn.cell.can_layer_bis <span class="im">import</span> CANLayer</span></code></pre></div>
</div>
<div class="cell markdown">
<p>If GPU's are available, we will make use of them. Otherwise, this
will run on CPU.</p>
</div>
<div class="cell code" data-execution_count="2">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>cpu
</code></pre>
</div>
</div>
<section id="pre-processing" class="cell markdown">
<h1>Pre-processing</h1>
<h2 id="import-data">Import data</h2>
<p>The first step is to import the dataset, shrec16, a benchmark dataset
for 3D mesh classification. We then lift each graph into our domain of
choice, a cell complex.</p>
<p>We also retrieve:</p>
<ul>
<li>input signals <code>x_0</code>,<code>x_1</code>, and
<code>x_2</code> on the nodes (0-cells), edges (1-cells), and faces
(2-cells) for each complex: these will be the model's inputs,</li>
<li>a binary classification label <code>y</code> associated to the cell
complex.</li>
</ul>
</section>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>shrec, _ <span class="op">=</span> datasets.mesh.shrec_16(size<span class="op">=</span><span class="st">&quot;small&quot;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>shrec <span class="op">=</span> {key: np.array(value) <span class="cf">for</span> key, value <span class="kw">in</span> shrec.items()}</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x_0s <span class="op">=</span> shrec[<span class="st">&quot;node_feat&quot;</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>x_1s <span class="op">=</span> shrec[<span class="st">&quot;edge_feat&quot;</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>x_2s <span class="op">=</span> shrec[<span class="st">&quot;face_feat&quot;</span>]</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> shrec[<span class="st">&quot;label&quot;</span>]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>simplexes <span class="op">=</span> shrec[<span class="st">&quot;complexes&quot;</span>]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Loading dataset...

done!
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="4">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>i_complex <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The </span><span class="sc">{</span>i_complex<span class="sc">}</span><span class="ss">th simplicial complex has </span><span class="sc">{</span>x_0s[i_complex]<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> nodes with features of dimension </span><span class="sc">{</span>x_0s[i_complex]<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">.&quot;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The </span><span class="sc">{</span>i_complex<span class="sc">}</span><span class="ss">th simplicial complex has </span><span class="sc">{</span>x_1s[i_complex]<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> edges with features of dimension </span><span class="sc">{</span>x_1s[i_complex]<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">.&quot;</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The </span><span class="sc">{</span>i_complex<span class="sc">}</span><span class="ss">th simplicial complex has </span><span class="sc">{</span>x_2s[i_complex]<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> faces with features of dimension </span><span class="sc">{</span>x_2s[i_complex]<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">.&quot;</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The </span><span class="sc">{</span>i_complex<span class="sc">}</span><span class="ss">th simplicial complex has label </span><span class="sc">{</span>ys[i_complex]<span class="sc">}</span><span class="ss">.&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The 6th simplicial complex has 252 nodes with features of dimension 6.
The 6th simplicial complex has 750 edges with features of dimension 10.
The 6th simplicial complex has 500 faces with features of dimension 7.
The 6th simplicial complex has label 9.
</code></pre>
</div>
</div>
<section
id="lift-into-cell-complex-domain-and-define-neighborhood-structures"
class="cell markdown">
<h2>Lift into cell complex domain and define neighborhood
structures</h2>
<p>We lift each simplicial complex into a cell complex.</p>
<p>Then, we retrieve the neighborhood structures (i.e. their
representative matrices) taht we will use to send messages on each cell
complex. In th case of this architecture we need the down and up
laplacians acting on 1-cells denoted by <span
class="math inline"><em>L</em><sub>↓,1</sub>, <em>L</em><sub>↑,1</sub></span></p>
</section>
<div class="cell code" data-execution_count="5">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>cc_list <span class="op">=</span> []</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>down_laplacian_list <span class="op">=</span> []</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>up_laplacian_list <span class="op">=</span> []</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> simplex <span class="kw">in</span> simplexes:</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    cell_complex <span class="op">=</span> simplex.to_cell_complex()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    cc_list.append(cell_complex)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    down_laplacian <span class="op">=</span> cell_complex.down_laplacian_matrix(rank<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    up_laplacian <span class="op">=</span> cell_complex.up_laplacian_matrix(rank<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    down_laplacian <span class="op">=</span> torch.from_numpy(down_laplacian.todense()).to_sparse()</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    up_laplacian <span class="op">=</span> torch.from_numpy(up_laplacian.todense()).to_sparse()</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    down_laplacian_list.append(down_laplacian)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    up_laplacian_list.append(up_laplacian)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>i_complex <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The </span><span class="sc">{</span>i_complex<span class="sc">}</span><span class="ss">th cell complex has a down_laplacian matrix of shape </span><span class="sc">{</span>down_laplacian_list[i_complex]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">.&quot;</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The </span><span class="sc">{</span>i_complex<span class="sc">}</span><span class="ss">th cell complex has an up_laplacian matrix of shape </span><span class="sc">{</span>up_laplacian_list[i_complex]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">.&quot;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The 6th cell complex has a down_laplacian matrix of shape torch.Size([750, 750]).
The 6th cell complex has an up_laplacian matrix of shape torch.Size([750, 750]).
</code></pre>
</div>
</div>
<section id="define-neighborhood-structures" class="cell markdown">
<h2>Define neighborhood structures.</h2>
<p>Implementing the CAN architecture will require to perform message
passing along neighborhood structures of the cell complexes.</p>
<p>Thus, now we retrieve these neighborhood structures (i.e. their
representative matrices) that we will use to send messages.</p>
<p>For the CAN, we need the down Laplacian matrix <span
class="math inline"><em>L</em><sub>↓,1</sub></span> and the up Laplacian
matrix <span class="math inline"><em>L</em><sub>↑,1</sub></span> of each
cell complex.</p>
</section>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>up_laplacian_list <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>down_laplacian_list <span class="op">=</span> []</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cell_complex <span class="kw">in</span> cc_list:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    up_laplacian <span class="op">=</span> cell_complex.up_laplacian_matrix(rank<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    down_laplacian <span class="op">=</span> cell_complex.down_laplacian_matrix(rank<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    up_laplacian <span class="op">=</span> torch.from_numpy(up_laplacian.todense()).to_sparse()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    down_laplacian <span class="op">=</span> torch.from_numpy(down_laplacian.todense()).to_sparse()</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    up_laplacian_list.append(up_laplacian)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    down_laplacian_list.append(down_laplacian)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>i_cc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Up Laplacian of the </span><span class="sc">{</span>i_cc<span class="sc">}</span><span class="ss">-th complex: </span><span class="sc">{</span>up_laplacian_list[i_cc]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">.&quot;</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Down Laplacian of the </span><span class="sc">{</span>i_cc<span class="sc">}</span><span class="ss">-th complex: </span><span class="sc">{</span>down_laplacian_list[i_cc]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">.&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Up Laplacian of the 0-th complex: torch.Size([750, 750]).
Down Laplacian of the 0-th complex: torch.Size([750, 750]).
</code></pre>
</div>
</div>
<section id="create-the-neural-network" class="cell markdown">
<h1>Create the Neural Network</h1>
<p>Using the CANLayer class, we create a neural network which applies a
CAN layer to the edges followed by linear layers on nodes, edges, and
faces.</p>
</section>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>in_channels_0 <span class="op">=</span> x_0s[<span class="dv">0</span>].shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>in_channels_1 <span class="op">=</span> x_1s[<span class="dv">0</span>].shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>in_channels_2 <span class="op">=</span> x_2s[<span class="dv">0</span>].shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The dimension of input features on nodes, edges and faces are: </span><span class="sc">{</span>in_channels_0<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>in_channels_1<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>in_channels_2<span class="sc">}</span><span class="ss">.&quot;</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The dimension of input features on nodes, edges and faces are: 6, 10 and 7.
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CAN(torch.nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;CAN.</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    in_channels_0 : int</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Dimension of input features on nodes.</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    in_channels_1 : int</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Dimension of input features on edges.</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">    in_channels_2 : int</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Dimension of input features on faces.</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">    num_classes : int</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Number of classes.</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">    n_layers : int</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Number of CAN layers.</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co">    att : bool</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Whether to use attention.</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        in_channels_0,</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        in_channels_1,</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        in_channels_2,</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        num_classes,</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        n_layers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        att<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layers):</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>            layers.append(CANLayer(channels<span class="op">=</span>in_channels_1, att<span class="op">=</span>att))</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> layers</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin_0 <span class="op">=</span> torch.nn.Linear(in_channels_0, num_classes)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin_1 <span class="op">=</span> torch.nn.Linear(in_channels_1, num_classes)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin_2 <span class="op">=</span> torch.nn.Linear(in_channels_2, num_classes)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x_0, x_1, x_2, down_laplacian, up_laplacian):</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Forward computation through layers, then linear layers, then avg pooling.</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="co">        ----------</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="co">        x_0 : torch.Tensor, shape = [n_nodes, in_channels_0]</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co">            Input features on the nodes (0-cells).</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="co">        x_1 : torch.Tensor, shape = [n_edges, in_channels_1]</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="co">            Input features on the edges (1-cells).</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a><span class="co">        x_2 : torch.Tensor, shape = [n_faces, in_channels_2]</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="co">            Input features on the faces (2-cells).</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="co">        down_laplacian : tensor, shape = [n_edges, n_edges]</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a><span class="co">            Down Laplacian of rank 1.</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="co">        up_laplacian : tensor, shape = [n_edges, n_edges]</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a><span class="co">            Up Laplacian of rank 1.</span></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a><span class="co">        -------</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="co">        _ : tensor, shape = [1]</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a><span class="co">            Label assigned to whole complex.</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>            x_1 <span class="op">=</span> layer(x_1, down_laplacian, up_laplacian)</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>        x_0 <span class="op">=</span> <span class="va">self</span>.lin_0(x_0)</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>        x_1 <span class="op">=</span> <span class="va">self</span>.lin_1(x_1)</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        x_2 <span class="op">=</span> <span class="va">self</span>.lin_2(x_2)</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Take the average of the 2D, 1D and 0D cell features. If they are NaN, convert them to 0.</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>        two_dimensional_cells_mean <span class="op">=</span> torch.nanmean(x_2, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>        two_dimensional_cells_mean[torch.isnan(two_dimensional_cells_mean)] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>        one_dimensional_cells_mean <span class="op">=</span> torch.nanmean(x_1, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>        one_dimensional_cells_mean[torch.isnan(one_dimensional_cells_mean)] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>        zero_dimensional_cells_mean <span class="op">=</span> torch.nanmean(x_0, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>        zero_dimensional_cells_mean[torch.isnan(zero_dimensional_cells_mean)] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the sum of the averages</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>            one_dimensional_cells_mean</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> zero_dimensional_cells_mean</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> two_dimensional_cells_mean</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
</div>
<section id="train-the-neural-network" class="cell markdown">
<h1>Train the Neural Network</h1>
<p>We specify the model, initialize loss, and specify an optimizer. We
first try it without any attention mechanism.</p>
</section>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CAN(in_channels_0, in_channels_1, in_channels_2, num_classes<span class="op">=</span><span class="dv">1</span>, n_layers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>crit <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.MSELoss()</span></code></pre></div>
</div>
<div class="cell markdown">
<p>We split the dataset into train and test sets.</p>
</div>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x_0_train, x_0_test <span class="op">=</span> train_test_split(x_0s, test_size<span class="op">=</span>test_size, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>x_1_train, x_1_test <span class="op">=</span> train_test_split(x_1s, test_size<span class="op">=</span>test_size, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>x_2_train, x_2_test <span class="op">=</span> train_test_split(x_2s, test_size<span class="op">=</span>test_size, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>up_laplacian_train, up_laplacian_test <span class="op">=</span> train_test_split(</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    up_laplacian_list, test_size<span class="op">=</span>test_size, shuffle<span class="op">=</span><span class="va">False</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>down_laplacian_train, down_laplacian_test <span class="op">=</span> train_test_split(</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    down_laplacian_list, test_size<span class="op">=</span>test_size, shuffle<span class="op">=</span><span class="va">False</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> train_test_split(ys, test_size<span class="op">=</span>test_size, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell markdown">
<p>We train the CAN using 10 epochs: we keep training minimal for the
purpose of rapid testing.</p>
</div>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>test_interval <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch_i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> []</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x_0, x_1, x_2, down_laplacian, up_laplacian, y <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        x_0_train,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        x_1_train,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        x_2_train,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        down_laplacian_train,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        up_laplacian_train,</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        x_0, x_1, x_2, y <span class="op">=</span> (</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>            torch.tensor(x_0).<span class="bu">float</span>().to(device),</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            torch.tensor(x_1).<span class="bu">float</span>().to(device),</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>            torch.tensor(x_2).<span class="bu">float</span>().to(device),</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>            torch.tensor(y).<span class="bu">float</span>().to(device),</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        down_laplacian, up_laplacian <span class="op">=</span> down_laplacian.<span class="bu">float</span>().to(</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>            device</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        ), up_laplacian.<span class="bu">float</span>().to(device)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> model(x_0, x_1, x_2, down_laplacian, up_laplacian)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_hat, y)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        epoch_loss.append(loss.item())</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;Epoch: </span><span class="sc">{</span>epoch_i<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(epoch_loss)<span class="sc">:.4f}</span><span class="ss">&quot;</span>,</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        flush<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch_i <span class="op">%</span> test_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x_0, x_1, x_2, down_laplacian, up_laplcian, y <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>                x_0_test,</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>                x_1_test,</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>                x_2_test,</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>                down_laplacian_test,</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>                up_laplacian_test,</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>                y_test,</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>            ):</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>                x_0, x_1, x_2, y <span class="op">=</span> (</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(x_0).<span class="bu">float</span>().to(device),</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(x_1).<span class="bu">float</span>().to(device),</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(x_2).<span class="bu">float</span>().to(device),</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(y).<span class="bu">float</span>().to(device),</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>                down_laplacian, up_laplacian <span class="op">=</span> down_laplacian.<span class="bu">float</span>().to(</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>                    device</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>                ), up_laplacian.<span class="bu">float</span>().to(device)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>                y_hat <span class="op">=</span> model(x_0, x_1, x_2, down_laplacian, up_laplacian)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>                test_loss <span class="op">=</span> loss_fn(y_hat, y)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Test_loss: </span><span class="sc">{</span>test_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>C:\Users\abrah\anaconda3\envs\topological_2\Lib\site-packages\torch\nn\modules\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 1 loss: 89.3527
Epoch: 2 loss: 81.9469
Test_loss: 59.5947
Epoch: 3 loss: 81.1082
Epoch: 4 loss: 80.3476
Test_loss: 55.3188
Epoch: 5 loss: 79.6493
Epoch: 6 loss: 79.0137
Test_loss: 51.6529
Epoch: 7 loss: 78.4377
Epoch: 8 loss: 77.9167
Test_loss: 48.4984
Epoch: 9 loss: 77.4454
Epoch: 10 loss: 77.0191
Test_loss: 45.7851
</code></pre>
</div>
</div>
<section id="train-the-neural-network-with-attention"
class="cell markdown">
<h1>Train the Neural Network with Attention</h1>
</section>
<div class="cell markdown">
<p>Now we create a new neural network, that uses the attention
mechanism.</p>
</div>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CAN(</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    in_channels_0, in_channels_1, in_channels_2, num_classes<span class="op">=</span><span class="dv">1</span>, n_layers<span class="op">=</span><span class="dv">2</span>, att<span class="op">=</span><span class="va">True</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>crit <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.MSELoss()</span></code></pre></div>
</div>
<div class="cell markdown">
<p>We run the training for this neural network:</p>
</div>
<div class="cell code" data-execution_count="14">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>test_interval <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch_i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="op">=</span> []</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x_0, x_1, x_2, down_laplacian, up_laplacian, y <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        x_0_train,</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        x_1_train,</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        x_2_train,</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        down_laplacian_train,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        up_laplacian_train,</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        x_0, x_1, x_2, y <span class="op">=</span> (</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>            torch.tensor(x_0).<span class="bu">float</span>().to(device),</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>            torch.tensor(x_1).<span class="bu">float</span>().to(device),</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>            torch.tensor(x_2).<span class="bu">float</span>().to(device),</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>            torch.tensor(y).<span class="bu">float</span>().to(device),</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        down_laplacian, up_laplacian <span class="op">=</span> down_laplacian.<span class="bu">float</span>().to(</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>            device</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        ), up_laplacian.<span class="bu">float</span>().to(device)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> model(x_0, x_1, x_2, down_laplacian, up_laplacian)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_hat, y)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>        epoch_loss.append(loss.item())</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;Epoch: </span><span class="sc">{</span>epoch_i<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(epoch_loss)<span class="sc">:.4f}</span><span class="ss">&quot;</span>,</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        flush<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch_i <span class="op">%</span> test_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x_0, x_1, x_2, down_laplacian, up_laplcian, y <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>                x_0_test,</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>                x_1_test,</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>                x_2_test,</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>                down_laplacian_test,</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>                up_laplacian_test,</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>                y_test,</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>            ):</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>                x_0, x_1, x_2, y <span class="op">=</span> (</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(x_0).<span class="bu">float</span>().to(device),</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(x_1).<span class="bu">float</span>().to(device),</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(x_2).<span class="bu">float</span>().to(device),</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>                    torch.tensor(y).<span class="bu">float</span>().to(device),</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>                down_laplacian, up_laplacian <span class="op">=</span> down_laplacian.<span class="bu">float</span>().to(</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>                    device</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>                ), up_laplacian.<span class="bu">float</span>().to(device)</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>                y_hat <span class="op">=</span> model(x_0, x_1, x_2, down_laplacian, up_laplacian)</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>                test_loss <span class="op">=</span> loss_fn(y_hat, y)</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Test_loss: </span><span class="sc">{</span>test_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>, flush<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch: 1 loss: 96.6442
Epoch: 2 loss: 76.8734
Test_loss: 52.6675
Epoch: 3 loss: 76.2222
Epoch: 4 loss: 75.4701
Test_loss: 46.0780
Epoch: 5 loss: 74.7502
Epoch: 6 loss: 74.0797
Test_loss: 40.4300
Epoch: 7 loss: 73.4617
Epoch: 8 loss: 72.8948
Test_loss: 35.5483
Epoch: 9 loss: 72.3760
Epoch: 10 loss: 71.9013
Test_loss: 31.3371
</code></pre>
</div>
</div>
</body>
</html>



<!-- <iframe
    src="https://mlgeis-arxiv-subject-classifier-demo.hf.space"
    frameborder="0"
    width="780"
    height="800"
></iframe> -->

<!-- ### Inference Pipeline

- #### Preprocessing
    - The title is retrieved from the input ID via a call to the arXiv API.
    - The title string is cleaned--inline LaTeX is removed, all accented characters are converted to their ASCII equivalents, LaTeX environments not enclosed by $ signs are removed.
- #### Huggingface Model
    - The model consists of a pretrained `bert-base-uncased` transformer and a single layer neural network 'classification head'. 
    - The input sentence (clean title) is encoded using this BERT model and its pooled output is run through the classification head. 
- #### Postprocessing
    - The classification head outputs a vector whose $$j^{th}$$ component is the probability that the $$j^{th}$$ label is relevant.
    - A minimum probability threshold required to predict a label is passed in and the vector of subject tag probabilities is converted into a list of english named subject tags.

#### Model Training

The huggingface model is an example of a "fine-tuned" model. The transformer model responsible for generating the vector representation of the input text is pretrained, and these weights are not modified. However, the classification head is trained on a dataset consisting of about 40,000 (title,label) pairs. This dataset was extracted from a random sample of 10% of all of the math articles contained in the [full arXiv kaggle dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) -->

<a id="HousingPrices"></a>

<!-- ## A Nearest-Neighbor Based Recommender System for Math Articles

Below is a content based recommender for arXiv math articles. It returns the 5 most similar math articles to an input. See below for a discussion of how it works.

<iframe
    src="https://mlgeis-ArXivRecommenderSystem.hf.space"
    frameborder="0"
    width="780"
    height="900"
></iframe>

### Model Design
- We start with a chosen library of articles from which we make recommendations. Using their title and abstract, every article
is embedded in a vector space in such a way that papers pointing 'in the same direction' are similar in content. To make recommendations, the input article is embedded in the same way and its 5 nearest neighbors in the library are calculated.

- We use the [allenai-specter sentence transformer](https://www.kaggle.com/datasets/Cornell-University/arxiv) as our embedding model. This model has been pre-trained on a large corpus of scientific articles specifically for the task of detecting semantic similarity.

### Unsupervised Topic Modeling
- Additionally, we performed unsupervised topic modeling on our recommendation library using [BERTopic.](https://maartengr.github.io/BERTopic/index.html) BERTopic works by applying the [HDBSCAN.](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) clustering algorithm on the embedded articles. A variant of tf-idf is then run on each cluster to extract the common topics present. Here is a picture of the embedding space after applying the [UMAP](https://umap-learn.readthedocs.io/en/latest/) dimension reduction. Keywords for the strongest topic clusters are shown on the right.

![Topic Modeling](/images/topicmodeling.png "Visualization of clusters and corresponding topics after dimension reduction.") -->


<a id="cs231"></a>

<!-- ## Stanford CS229 "Intro to Machine Learning" Personal Notes and Problem Set Solutions

The following is a collection of notes and exercises written while following the Stanford CS 229 "Intro to Machine Learning" course. [Full repository of course materials can be found here.](https://github.com/maxim5/cs229-2018-autumn) All lectures are available on [YouTube.](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

- #### Problem Set 0
    - [Gradients and Hessians](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS0/PS0-1.ipynb)
    - [Positive Definite Matrices](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS0/PS0-1.ipynb)
    - [Eigenvectors and Eigenvalues](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS0/PS0-1.ipynb)
- #### Problem Set 1
    - [Logistic Regression & GDA](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS0/PS0-1.ipynb)
    - [Logistic Regression with Incomplete Positive-Only Labels](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS1/PS1-2.ipynb)
    - [Poisson Regression](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS1/PS1-3.ipynb)
    - [Convexity of Generalized Linear Models](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS1/PS1-3.ipynb)
    - [Locally Weighted Linear Regression](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS1/PS1-3.ipynb)
- #### Problem Set 2
    - [Training Stability of Logistic Regression](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-1.ipynb)
    - [Model Calibration](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-2.ipynb)
    - [Notes on Bayesian Statistics and the Bayesian Interpretation of Regularization](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-3.ipynb)
    - [The Kernel Trick: Construction of Mercer Kernels](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-4.ipynb)
    - [An Implementation of the Perceptron Classifier with the Kernel Trick](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-5.ipynb)
    - [Naive Bayes for Binary Text Classification](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-6.ipynb)
- #### Problem Set 3
    - [Properties of the Kullback-Liebler Divergence](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS2/PS2-6.ipynb)
    - [Kullback-Liebler Divergence, Fisher Information, and the Natural Gradient](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS3/PS3%20Solutions/PS3-3%20KL%20divergence%2C%20Fisher%20information%2C%20natural%20gradient.ipynb)
    - [Expectation Maximization Algorithm and a Semi-Supervised Variant](https://github.com/Michael-Geis/CS-229-F18-Solutions/blob/main/PS3/PS3%20Solutions/PS3-4%20Semi-supervised%20EM.ipynb) --> -->
